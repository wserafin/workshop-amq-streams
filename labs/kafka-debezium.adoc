## Kafka Connect


#### Step 1:  Installing a database

in your OpenShift project deploy a MySql database

```
oc new-app --name=mysql debezium/example-mysql:0.9

oc set env dc/mysql MYSQL_ROOT_PASSWORD=debezium  MYSQL_USER=mysqluser MYSQL_PASSWORD=mysqlpw
```


#### Step 2: Creation of the Kafka Connect Cluster

Create a Kubernetes secret with connectivity information for the MySqL database:
```
apiVersion: v1
data:
  debezium-mysql-credentials.properties: bXlzcWxfdXNlcm5hbWU6IGRlYmV6aXVtCm15c3FsX3Bhc3N3b3JkOiBkYnoK
kind: Secret
metadata:
  name: my-sql-credentials
type: Opaque
```

```
oc apply -f https://raw.githubusercontent.com/masauve/AMQStreams-workshop/master/manifests/my-sql-credentials.yaml
```


Using the AMQ Streams operator, create a Kafka Connect cluster.

```
apiVersion: kafka.strimzi.io/v1beta1
kind: KafkaConnect
metadata:
  name: my-connect-cluster
  annotations:
  # use-connector-resources configures this KafkaConnect
  # to use KafkaConnector resources to avoid
  # needing to call the Connect REST API directly
    strimzi.io/use-connector-resources: "true"
spec:
  image: quay.io/msauve/kafkaconnect
  replicas: 1
  bootstrapServers: production-ready-kafka-bootstrap:9093
  tls:
    trustedCertificates:
      - secretName: production-ready-cluster-ca-cert
        certificate: ca.crt
  config:
    config.storage.replication.factor: 1
    offset.storage.replication.factor: 1
    status.storage.replication.factor: 1
    config.providers: file
    config.providers.file.class: org.apache.kafka.common.config.provider.FileConfigProvider
  externalConfiguration:
    volumes:
      - name: connector-config
        secret:
          secretName: my-sql-credentials
```
```
oc apply -f https://raw.githubusercontent.com/masauve/AMQStreams-workshop/master/manifests/kafka-connect.yaml
```
Wait for the Kafka Connect pods to start:

```
oc get pods 
```
![Kafka Connect](images/ss-running-pods.png)


#### Step 3: Creation of a Kafka Connect connector with the AMQStreams Operator

Run the following script (MacOS, Bash or Linux) in a "Terminal" window to create a Kafka-Connect event source.

```
apiVersion: "kafka.strimzi.io/v1alpha1"
kind: "KafkaConnector"
metadata:
  name: "inventory-connector"
  labels:
    strimzi.io/cluster: my-connect-cluster
spec:
  class: io.debezium.connector.mysql.MySqlConnector
  tasksMax: 1
  config:
    database.hostname: mysql
    database.port: "3306"
    database.user: "${file:/opt/kafka/external-configuration/connector-config/debezium-mysql-credentials.properties:mysql_username}"
    database.password: "${file:/opt/kafka/external-configuration/connector-config/debezium-mysql-credentials.properties:mysql_password}"
    database.server.id: "184054"
    database.server.name: "dbserver1"
    database.whitelist: "inventory"
    database.history.kafka.bootstrap.servers: "production-ready-kafka-bootstrap:9092"
    database.history.kafka.topic: "schema-changes.inventory"
    include.schema.changes: "true" 
```

```
oc apply -f https://raw.githubusercontent.com/masauve/AMQStreams-workshop/master/manifests/kafka-connector.yaml
```

#### Step 4: Test the connector installation

The connector should have created several Topics on your Kafka cluster. These topics represent the structure of your database:

![Kafka Connect](images/topics-kc.png)


Open a Kafka customer on the inventory.customers topic:

```
oc run kafka-consumer -ti --image=registry.access.redhat.com/amq7/amq-streams-kafka:1.1.0-kafka-2.1.1 --rm=true --restart=Never -- bin/kafka-console-consumer.sh --bootstrap-server production-ready-kafka-bootstrap:9092     --property print.key=true --topic dbserver1.inventory.customers --from-beginning
```

In the Openshift console, open a terminal inside the MySql Pod:

![MySQL](images/lab5-connect-04.png)

* Project: votre projet
* Workloads: Pods
* MySQL pod
* Terminal Tab

In the terminal, connect to MySQl with the mysql client installed in the container:

```
mysql -u mysqluser -p 
```

* password: mysqlpw

Select the "Inventory" database:

```
use inventory;
```

Insert data (for example):

```
insert into customers values (null,'Wojciech', 'Serafin', 'ws@me.com');
```


The content of the comic should be automatically sent by Kafka-Connect on the Kafka topic (dbserver1.inventory.customers)

![resultat](images/lab5-connect-red.png)
