=== Login on the OpenShift Console with your provided user name.
For this workshop, the console is available here:
http://console-openshift-console.apps.cluster-telus-d491.telus-d491.example.opentlc.com
User and password assigned 

Optional: Download OpenShift client tool (oc) from here: http://console-openshift-console.apps.cluster-telus-d491.telus-d491.example.opentlc.com

using the dowloaded oc command, login to OpenShift using the command line tool.
The cluster is configured using OAuth authentication, you need to obtain your token from the console:

image::images/openshiftlogin.png[]

oc login --token=gTZkG-OKZ9OWkoEAR25vbXgtZNcbWE-YLPadsaVKMz83M --server=https://api.cluster-telus-d491.telus-d491.example.opentlc.com:6443

Create your first OpenShift project:

image::images/openshiftproject.png[]

or from the command line:

oc new-project [project-name] - Use your assigned userID in your project name example--> amq-user1

you will be running all the labs from that project

=== Inspect the AMQ Streams Operator

in your project, you should see that the AMQStreams operator was made available to you by a cluster administrator:
NOTE: Give a few seconds for operator to show up in your project.

image::images/openshiftoperators.png[]

if you click on the Operator, you should see that it provides several APIs to interact with  Apache Kafka:

image::images/operatordetails.png[]


=== Creating an Apache Kafka cluster

It is time to start an Apache Kafka cluster.
We will create now the most basic cluster possible.
The configuration file is https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/clusters/simple-cluster.yaml[here].
You can open it - it looks like this:

----
apiVersion: kafka.strimzi.io/v1alpha1
kind: Kafka
metadata:
  name: simple-cluster
spec:
  kafka:
    replicas: 1
    listeners:
      plain: {}
      tls: {}
    config:
      offsets.topic.replication.factor: 1
      transaction.state.log.replication.factor: 1
      transaction.state.log.min.isr: 1
    storage:
      type: ephemeral
  zookeeper:
    replicas: 1
    storage:
      type: ephemeral
  entityOperator:
    topicOperator: {}
    userOperator: {}
----

Now let's create the cluster by deploying this new custom resource:
----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/clusters/simple-cluster.yaml
----

Again, follow the deployment from the OpenShift console.
You should see three separate deployments:

* `simple-cluster-zookeeper` - a stateful set containing the Zookeeper ensemble
* `simple-cluster-kafka` - a stateful set containing the Kafka cluster
* `simple-cluster-entity-operator` - a deployment containing the entity operator for managing topics and users

=== Testing the deployment

Now, let's quickly test that the deployed Kafka cluster works.
Let's log into one of the cluster pods:

----
$ oc rsh simple-cluster-kafka-0
----

Next, let's start a producer:

----
$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic test-topic
----

Once the console producer is started, enter a few values:

----
> test
> test2
----

(Do not worry if you see the warnings below.
They are part of the interaction and indicate that the topic has not been found and broker will autocreate the `test-topic`.
The message `test` will be properly received by Kafka).

----
OpenJDK 64-Bit Server VM warning: If the number of processors is expected to increase from one, then you should configure the number of parallel GC threads appropriately using -XX:ParallelGCThreads=N
>test
[2019-02-05 15:32:46,828] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 1 : {test-topic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
[2019-02-05 15:32:46,939] WARN [Producer clientId=console-producer] Error while fetching metadata with correlation id 3 : {test-topic=LEADER_NOT_AVAILABLE} (org.apache.kafka.clients.NetworkClient)
>test2
----

Now let's open another terminal into the cluster pod in a separate terminal (oc login --token...)

----
$ oc rsh --container kafka simple-cluster-kafka-0
----

And let's start a consumer:

----
bin/kafka-console-consumer.sh --bootstrap-server localhost:9092 --topic test-topic --from-beginning
----

Once the consumer is started, you should see the previously sent messages in the output.
Reverting to the terminal where we started the console producer and sending any new messages there will result in those messages being displayed in the consumer terminal.

Now let's stop both producer and consumer applications with `CTRL-C` and then exit from the terminal of both containers.

----
exit
----

=== Kafka clusters and Kafka resources

The Kafka resource we just created is a representation of the running Kafka cluster.
You can use it to inspect and modify the current cluster configuration.
For example:

----
oc get kafka simple-cluster -o yaml
----

Will yield a detailed representation of the resource on the cluster:

----
apiVersion: kafka.strimzi.io/v1alpha1
kind: Kafka
metadata:
  annotations:
    kubectl.kubernetes.io/last-applied-configuration: |
      {"apiVersion":"kafka.strimzi.io/v1alpha1","kind":"Kafka","metadata":{"annotations":{},"name":"simple-cluster","namespace":"amq-streams"},"spec":{"entityOperator":{"topicOperator":{},"userOperator":{}},"kafka":{"config":{"offsets.topic.replication.factor":1,"transaction.state.log.min.isr":1,"transaction.state.log.replication.factor":1},"listeners":{"plain":{},"tls":{}},"replicas":1,"storage":{"type":"ephemeral"}},"zookeeper":{"replicas":1,"storage":{"type":"ephemeral"}}}}
  creationTimestamp: 2019-02-05T15:27:11Z
  generation: 1
  name: simple-cluster
  namespace: amq-streams
  resourceVersion: "136009"
  selfLink: /apis/kafka.strimzi.io/v1alpha1/namespaces/amq-streams/kafkas/simple-cluster
  uid: 81e3ddbe-295a-11e9-bbf1-2cabcdef0010
spec:
  entityOperator:
    topicOperator: {}
    userOperator: {}
  kafka:
    config:
      offsets.topic.replication.factor: 1
      transaction.state.log.min.isr: 1
      transaction.state.log.replication.factor: 1
    listeners:
      plain: {}
      tls: {}
    replicas: 1
    storage:
      type: ephemeral
  zookeeper:
    replicas: 1
    storage:
      type: ephemeral
----

Finally, let's delete the Kafka cluster.
We will replace it with a configuration that is more appropriate for real world use cases.

----
oc delete kafka simple-cluster
----

=== Conclusion

In this workshop module, you have:

* Reviewed the AMQ Streams installation
* Deployed a simple Kafka cluster
* Run a producer and consumer to validate the settings
