== MirrorMaker

This lab walks through setting up MirrorMaker for replicating messages between different clusters.

=== What does MirrorMaker do?

Often, applications need to communicate between each other across Kafka clusters.
For example, data might be ingested in Kafka in a data center and consumed in another data center, for reasons such as locality.
In this lab we will show how data can be replicated between Kafka clusters using MirrorMaker.

First of all, if the `timer-producer` and `log-consumer` applications are still running, let's stop them.

----
oc delete deployment timer-producer
oc delete deployment log-consumer
----

Check on the OpenShift Web console that the related pods are not running anymore.

=== Setting up the source and target clusters

We will use the cluster previously created in this workshop as source but without the authentication enabled.
In order to do that, let's update the already running `production-ready` cluster actually removing the authentication.

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/clusters/production-ready.yaml
----

We will use this cluster as source of the mirroring, having the `timer-producer` application sending messages to the `lines` topic.

Let's deploy another `production-ready-target` cluster as target of the mirroring from where the `log-consumer` application will read messages.

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/clusters/production-ready-target.yaml
----

WARNING !
Wait for the new cluster to be deployed.
Use -->
----
oc get pods
----
To verify that all pods are up and runing 

Now, because the `timer-producer` application will write on the already existing `lines` topic on `production-ready` cluster, let's create a corresponding `lines` topic on the `production-ready-target` cluster as destination of the mirroring, from where the `log-consumer` application will read messages.

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/topics/lines-10-target.yaml
----

Now let's deploy MirrorMaker 2.

----
oc apply -f https://raw.githubusercontent.com/redhat-canada/workshop-amq-streams/master/configurations/clusters/mirror-maker-single-namespace.yaml
----

The notions of producer and consumer are from MirrorMaker's perspective.
Messages will be read from the producer (in MirrorMaker config) and published to consumer.

Now let's deploy the `log-consumer` application reading from the target cluster.

----
oc apply -f https://raw.githubusercontent.com/redhat-canada/workshop-amq-streams/master/configurations/applications/log-consumer-target.yaml
----

And finally the `timer-producer` application writing to the source cluster.

----
oc apply -f https://raw.githubusercontent.com/RedHatWorkshops/workshop-amq-streams/master/configurations/applications/timer-producer.yaml
----

Logging the related pods should yield the expected results and data flows between systems.

----
oc get pods | grep consumer
logs -f log-consumer-<$id>  -c log-consumer --tail=1
-----
You should see messages:
020-06-18 19:47:50.360  INFO 1 --- [r-source.lines]] o.a.k.c.consumer.internals.Fetcher       : [Consumer clientId=consumer-1, groupId=test-group] Resetting offset for partition my-cluster-source.lines-0 to offset 0.
2020-06-18 19:49:09.967  INFO 1 --- [r-source.lines]] route1                                   : Message 3 at Thu Jun 18 19:27:40 UTC 2020
2020-06-18 19:49:09.977  INFO 1 --- [r-source.lines]] route1                                   : Message 9 at Thu Jun 18 19:40:02 UTC 2020

